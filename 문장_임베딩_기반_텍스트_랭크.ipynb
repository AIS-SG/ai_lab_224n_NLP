{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "문장 임베딩 기반 텍스트 랭크.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIS-SG/ai_lab_224n_NLP/blob/main/%EB%AC%B8%EC%9E%A5_%EC%9E%84%EB%B2%A0%EB%94%A9_%EA%B8%B0%EB%B0%98_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%9E%AD%ED%81%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUyLpIePfzka"
      },
      "source": [
        "# 딥러닝을 이용한 자연어 처리 입문 - 문장 임베딩 기반 텍스트 랭크\n",
        "##### 참고자료 : https://wikidocs.net/91051"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLz82YEpaCuJ"
      },
      "source": [
        "## 텍스트랭크\n",
        "##### 텍스트랭크 알고리즘은 검색 엔진에서 웹 페이지의 순위를 정하기 위해 사용되던 알고리즘이다.\n",
        "##### 텍스트랭크에서 그래프의 노드들은 문장들이며, 각 간선의 가중치는 문장들 간의 유사도를 의미\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q4t46WmIM7_"
      },
      "source": [
        "## 사전 훈련된 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X75Tt0QDJ9M"
      },
      "source": [
        "import numpy as np\n",
        "import gensim\n",
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSYBsHZ0ITXS"
      },
      "source": [
        "# 사전 훈련된 GloVe 다운로드\n",
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall() \n",
        "zf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XorkIKfarENZ"
      },
      "source": [
        "glove_dict = dict()\n",
        "f = open('glove.6B.100d.txt', encoding=\"utf8\") # 100차원의 GloVe 벡터를 사용\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 100개의 값을 가지는 array로 변환\n",
        "    glove_dict[word] = word_vector_arr\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL2HEA7ZZxIo",
        "outputId": "f30547fa-19c8-438f-aa38-6e8c135eacca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "glove_dict['cat']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.23088  ,  0.28283  ,  0.6318   , -0.59411  , -0.58599  ,\n",
              "        0.63255  ,  0.24402  , -0.14108  ,  0.060815 , -0.7898   ,\n",
              "       -0.29102  ,  0.14287  ,  0.72274  ,  0.20428  ,  0.1407   ,\n",
              "        0.98757  ,  0.52533  ,  0.097456 ,  0.8822   ,  0.51221  ,\n",
              "        0.40204  ,  0.21169  , -0.013109 , -0.71616  ,  0.55387  ,\n",
              "        1.1452   , -0.88044  , -0.50216  , -0.22814  ,  0.023885 ,\n",
              "        0.1072   ,  0.083739 ,  0.55015  ,  0.58479  ,  0.75816  ,\n",
              "        0.45706  , -0.28001  ,  0.25225  ,  0.68965  , -0.60972  ,\n",
              "        0.19578  ,  0.044209 , -0.31136  , -0.68826  , -0.22721  ,\n",
              "        0.46185  , -0.77162  ,  0.10208  ,  0.55636  ,  0.067417 ,\n",
              "       -0.57207  ,  0.23735  ,  0.4717   ,  0.82765  , -0.29263  ,\n",
              "       -1.3422   , -0.099277 ,  0.28139  ,  0.41604  ,  0.10583  ,\n",
              "        0.62203  ,  0.89496  , -0.23446  ,  0.51349  ,  0.99379  ,\n",
              "        1.1846   , -0.16364  ,  0.20653  ,  0.73854  ,  0.24059  ,\n",
              "       -0.96473  ,  0.13481  , -0.0072484,  0.33016  , -0.12365  ,\n",
              "        0.27191  , -0.40951  ,  0.021909 , -0.6069   ,  0.40755  ,\n",
              "        0.19566  , -0.41802  ,  0.18636  , -0.032652 , -0.78571  ,\n",
              "       -0.13847  ,  0.044007 , -0.084423 ,  0.04911  ,  0.24104  ,\n",
              "        0.45273  , -0.18682  ,  0.46182  ,  0.089068 , -0.18185  ,\n",
              "       -0.01523  , -0.7368   , -0.14532  ,  0.15104  , -0.71493  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymWA8L06Z0at",
        "outputId": "d731006c-c769-4048-e99a-e6845d1f8e17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 사전 훈련된 FastText 다운로드\n",
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 1.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 1.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (50.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "fasttext\n",
            "yes\n",
            "cat\n",
            "fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3023565 sha256=1fad3e3dbad920b12796b21a27cbd9613765ba886652de2963f7ced8c6a0087e\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ3o7b8VZ3-b"
      },
      "source": [
        "# 300차원의 FastText 벡터 사용\n",
        "import fasttext.util\n",
        "fasttext.util.download_model('en', if_exists='ignore')\n",
        "ft = fasttext.load_model('cc.en.300.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tI-dzNwZ6ge"
      },
      "source": [
        "ft.get_word_vector('cat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRdatNEUrgXL"
      },
      "source": [
        "# 사전 훈련된 Word2Vec 다운로드\n",
        "# 300차원의 Word2Vec 벡터 사용\n",
        "urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
        "                           filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxjruTJ_rh5z"
      },
      "source": [
        "word2vec_model['cat']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETAOEHTuDKKJ"
      },
      "source": [
        "## 문장 임베딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS19z_aNI00f"
      },
      "source": [
        "##### 문장 벡터를 얻는 가장 간단한 문장에 존재하는 단어 벡터들의 평균을 구하는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwsxb6UVroPo"
      },
      "source": [
        "embedding_dim = 100\n",
        "zero_vector = np.zeros(embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mKDXPxDJSk9"
      },
      "source": [
        "##### 사전 훈련된 GloVe 벡터를 변환하면서, OOV 문제가 발생할 경우에는 해당 단어를 영벡터로 변환한다.\n",
        "##### 이렇게 모인 단어 벡터들의 평균을 구하여 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0naPhmmr2gc",
        "outputId": "514c9b29-6853-43e7-974f-7d78be68a257",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 단어 벡터의 평균으로부터 문장 벡터를 얻는다.\n",
        "def calculate_sentence_vector(sentence):\n",
        "  return sum([glove_dict.get(word, zero_vector) \n",
        "                  for word in sentence])/len(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "시작 토큰 번호 : [8178]\n",
            "종료 토큰 번호 : [8179]\n",
            "단어 집합의 크기 : 8180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeUkTwqir_g8",
        "outputId": "1072ed06-65c5-4097-a61b-aaddc0daa75d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "eng_sent = ['I', 'am', 'a', 'student']\n",
        "sentence_vector = calculate_sentence_vector(eng_sent)\n",
        "print(len(sentence_vector))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "임의의 질문 샘플을 정수 인코딩 : [5766, 611, 3509, 141, 685, 3747, 849]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWAYrv9asDiJ",
        "outputId": "180c6506-b4f0-4538-ed16-ef7c59cddf0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "kor_sent = ['전', '좋은', '학생', '입니다']\n",
        "sentence_vector = calculate_sentence_vector(kor_sent)\n",
        "print(sentence_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정수 인코딩 후의 문장 [5766, 611, 3509, 141, 685, 3747, 849]\n",
            "기존 문장: 가스비 비싼데 감기 걸리겠어\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgOgdlPCDRp4"
      },
      "source": [
        "## 텍스트 랭크를 이용한 텍스트 요약"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dun5I7KZ8UN"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from urllib.request import urlretrieve\n",
        "import zipfile\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEWUGOXVaIWD",
        "outputId": "c6b315e4-5871-4fbf-c54c-30e725b8021b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# NLTK에서 제공하는 불용어를 받아온다.\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "질문 데이터의 크기(shape) : (11823, 40)\n",
            "답변 데이터의 크기(shape) : (11823, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpOpPXKvaJvJ",
        "outputId": "a792e7d3-d826-4f53-b120-1142bbd33e7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "urlretrieve(\"https://raw.githubusercontent.com/prateekjoshi565/textrank_text_summarization/master/tennis_articles_v4.csv\", filename=\"tennis_articles_v4.csv\")\n",
        "data = pd.read_csv(\"tennis_articles_v4.csv\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8178 7915 4207 3060   41 8179    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKJcUKa2aLqq"
      },
      "source": [
        "data = data[['article_text']]\n",
        "data['sentences'] = data['article_text'].apply(sent_tokenize)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8cTEJCxaNCs",
        "outputId": "c8cf7709-bd35-44f1-b9e3-c8f4a7cb2485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 토큰화 함수\n",
        "def tokenization(sentences):\n",
        "    return [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "# 전처리 함수\n",
        "def preprocess_sentence(sentence):\n",
        "  # 영어를 제외한 숫자, 특수 문자 등은 전부 제거. 모든 알파벳은 소문자화\n",
        "  sentence = [re.sub(r'[^a-zA-z\\s]', '', word).lower() for word in sentence]\n",
        "  # 불용어가 아니면서 단어가 실제로 존재해야 한다.\n",
        "  return [word for word in sentence if word not in stop_words and word]\n",
        "\n",
        "# 위 전처리 함수를 모든 문장에 대해서 수행. 이 함수를 호출하면 모든 행에 대해서 수행.\n",
        "def preprocess_sentences(sentences):\n",
        "    return [preprocess_sentence(sentence) for sentence in sentences]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "[[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[3844   74 7894    1 8179    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK_DQTNI4BMh",
        "outputId": "559c9168-1c11-4b13-f7a5-bf538f01c92e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "data['tokenized_sentences'] = data['sentences'].apply(tokenization)\n",
        "data['tokenized_sentences'] = data['tokenized_sentences'].apply(preprocess_sentences)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-ebb50f3b6f77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mDROPOUT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m model = transformer(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transformer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDQD37pz4HuQ",
        "outputId": "8b1b9a69-d87c-41a2-a67c-1c9293c581b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "embedding_dim = 100\n",
        "zero_vector = np.zeros(embedding_dim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-cb9b04e5e4d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuaTl_qN4JYX"
      },
      "source": [
        "# 단어 벡터의 평균으로부터 문장 벡터를 얻는다.\n",
        "def calculate_sentence_vector(sentence):\n",
        "  if len(sentence) != 0:\n",
        "    return sum([glove_dict.get(word, zero_vector) \n",
        "                  for word in sentence])/len(sentence)\n",
        "  else:\n",
        "    return zero_vector\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKybIeq84LAh"
      },
      "source": [
        "# 각 문장에 대해서 문장 벡터를 반환\n",
        "def sentences_to_vectors(sentences):\n",
        "    return [calculate_sentence_vector(sentence) \n",
        "              for sentence in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmwdxxXz4OtH"
      },
      "source": [
        "data['SentenceEmbedding'] = data['tokenized_sentences'].apply(sentences_to_vectors)\n",
        "data[['SentenceEmbedding']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w-YnfxkTWoZ"
      },
      "source": [
        "# 문장 벡터들 간의 유사도를 구한 유사도 행렬 만들기, 행렬의 크기 (문장 개수 * 문장개)\n",
        "def similarity_matrix(sentence_embedding):\n",
        "  sim_mat = np.zeros([len(sentence_embedding), len(sentence_embedding)])\n",
        "  for i in range(len(sentence_embedding)):\n",
        "      for j in range(len(sentence_embedding)):\n",
        "        sim_mat[i][j] = cosine_similarity(sentence_embedding[i].reshape(1, embedding_dim),\n",
        "                                          sentence_embedding[j].reshape(1, embedding_dim))[0,0]\n",
        "  return sim_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRLxx9uqTYt1"
      },
      "source": [
        "data['SimMatrix'] = data['SentenceEmbedding'].apply(similarity_matrix)\n",
        "data['SimMatrix']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41xw0_2TTaWc"
      },
      "source": [
        "print('두번째 샘플의 문장 개수 :',len(data['tokenized_sentences'][1]))\n",
        "print('두번째 샘플의 문장 벡터가 모인 문장 행렬의 크기(shape) :',np.shape(data['SentenceEmbedding'][1]))\n",
        "print('두번째 샘플의 유사도 행렬의 크기(shape) :',data['SimMatrix'][1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxL05IngVkbM"
      },
      "source": [
        "# 유사도 행렬로부터 그래프 그리기\n",
        "def draw_graphs(sim_matrix):\n",
        "  nx_graph = nx.from_numpy_array(sim_matrix)\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  pos = nx.spring_layout(nx_graph)\n",
        "  nx.draw(nx_graph, with_labels=True, font_weight='bold')\n",
        "  nx.draw_networkx_edge_labels(nx_graph,pos,font_color='red')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdlhtlSUVkSb"
      },
      "source": [
        "draw_graphs(data['SimMatrix'][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZbg9HxRVkHN"
      },
      "source": [
        "# 페이지링크 알고리즘의 입력으로 사용하여 각 문장의 점수를 구한다.\n",
        "def calculate_score(sim_matrix):\n",
        "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqOIpFp3VxBy"
      },
      "source": [
        "data['score'] = data['SimMatrix'].apply(calculate_score)\n",
        "data[['SimMatrix', 'score']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VeS1l5nVyP9"
      },
      "source": [
        "data['score'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIVkA5OJVyGs"
      },
      "source": [
        "# 점수에 따라서 정렬 후에 상위 3개 문장만 반환\n",
        "def ranked_sentences(sentences, scores, n=3):\n",
        "    top_scores = sorted(((scores[i],s) \n",
        "                         for i,s in enumerate(sentences)), \n",
        "                                reverse=True)\n",
        "    top_n_sentences = [sentence \n",
        "                        for score,sentence in top_scores[:n]]\n",
        "    return \" \".join(top_n_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdqy-A-5VyAP"
      },
      "source": [
        "data['summary'] = data.apply(lambda x: \n",
        "                            ranked_sentences(x.sentences, \n",
        "                            x.score), axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YxdMJb2Vx5R"
      },
      "source": [
        "# 모든 문서에 대해서 원문과 요약문 출력\n",
        "for i in range(0, len(data)):\n",
        "  print(i+1,'번 문서')\n",
        "  print('원문 :',data.loc[i].article_text)\n",
        "  print('')\n",
        "  print('요약 :',data.loc[i].summary)\n",
        "  print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnZDnA_-VxzG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}