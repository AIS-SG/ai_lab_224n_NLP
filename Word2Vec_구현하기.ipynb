{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec 구현하기.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyN8tBS7NVgS2f6jJv9eZ329",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIS-SG/ai_lab_224n_NLP/blob/main/Word2Vec_%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J4x1wLR0g8N"
      },
      "source": [
        "# 딥러닝을 이용한 자연어 처리 입문 - Word2Vec 구현하기\n",
        "##### 참고 자료 : https://wikidocs.net/69141"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHgHbmJy0uvm"
      },
      "source": [
        "## 2. 20뉴스그룹 데이터 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skPUcQyB0Pl3"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmuDCcLA2v1q"
      },
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9icjZfkz1cK6"
      },
      "source": [
        "##### 하나의 샘플에는 최소 단어 2개는 있어야 한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S1XWxwg1H6h"
      },
      "source": [
        "#지속적으로 이를 만족하지 않는 샘플들을 제거하기\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "print('총 샘플 수 :',len(documents))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyxEwS171nhj"
      },
      "source": [
        "# 불필요한 토큰 제거, 소문자화를 통해 정규화 진행\n",
        "\n",
        "news_df = pd.DataFrame({'document':documents})\n",
        "# 특수 문자 제거\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apDOfWCO2BUY"
      },
      "source": [
        "# 현재 데이터프레임에 Null 값이 있는지 확인\n",
        "news_df.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBWByJzy2J0s"
      },
      "source": [
        "# Null 값이 없지만, 빈 값(empy) 유무도 확인해야한다.\n",
        "# 모든 빈 값을 Null 값으로 변환하고, 다시 Null 값이 있는지 확인\n",
        "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
        "news_df.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MShcQzn72Yzz"
      },
      "source": [
        "# Null 값 제거\n",
        "news_df.dropna(inplace=True)\n",
        "print('총 샘플 수 :',len(news_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E4iZ--p2id6"
      },
      "source": [
        "# 불용어를 제거\n",
        "stop_words = stopwords.words('english')\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "tokenized_doc = tokenized_doc.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoKwETvV2lfh"
      },
      "source": [
        "# 단어가 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거.\n",
        "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
        "tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n",
        "print('총 샘플 수 :',len(tokenized_doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZTjDQji3d9x"
      },
      "source": [
        "# 정수 인코딩 진행\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_doc)\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {v:k for k, v in word2idx.items()}\n",
        "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKoqKqAS3h_8"
      },
      "source": [
        "# 상위 2개의 샘플을 출력\n",
        "print(encoded[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOB_WMt53mDR"
      },
      "source": [
        "# 단어 집합의 크기 확인\n",
        "vocab_size = len(word2idx) + 1 \n",
        "print('단어 집합의 크기 :', vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf1JYed_338i"
      },
      "source": [
        "## 3. 네거티브 샘플링을 통한 데이터셋 구성하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v-lYaMw3_eI"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "# 네거티브 샘플링\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3yIy-Y-5Ru4"
      },
      "source": [
        "##### 윈도우 크기 내에서 중심 단어, 주변 단어의 관계를 가지는 경우에는 1의 레이블을 갖도록 하고, 그렇지 않은 경우는 0의 레이블을 가지도록하여 데이터셋을 구성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6Qtc4yE4hty"
      },
      "source": [
        "# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(5):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "          idx2word[pairs[i][0]], pairs[i][0], \n",
        "          idx2word[pairs[i][1]], pairs[i][1], \n",
        "          labels[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMUOzkKS5Pqe"
      },
      "source": [
        "print('전체 샘플 수 :',len(skip_grams))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgug0bqh5juN"
      },
      "source": [
        "# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
        "print(len(pairs))\n",
        "print(len(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUDGryj75osK"
      },
      "source": [
        "# 모든 뉴스그룹 샘플에 대해서 수행\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YuC4WZ46EzJ"
      },
      "source": [
        "## 4. SGNS 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6wnno9f6RyL"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
        "from tensorflow.keras.layers import Dot\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import SVG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX5eGcoS6YAb"
      },
      "source": [
        "# 임베딩 벡터의 차원를 100으로 정함(하이퍼파리미터)\n",
        "embed_size = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap-I2l706mvD"
      },
      "source": [
        "### 모델 설계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqovXlCK6hKC"
      },
      "source": [
        "# 중심 단어를 위한 임베딩 테이블\n",
        "w_inputs = Input(shape=(1, ), dtype='int32')\n",
        "word_embedding = Embedding(vocab_size, embed_size)(w_inputs)\n",
        "\n",
        "# 주변 단어를 위한 임베딩 테이블\n",
        "c_inputs = Input(shape=(1, ), dtype='int32')\n",
        "context_embedding  = Embedding(vocab_size, embed_size)(c_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG7Q_Ylw6wA7"
      },
      "source": [
        "##### 임베딩 테이블을 거쳐서 내적을 수행하고, 내적의 결과는 1또는 0을 예측하기 위해 시그모이드 함수를 활성화 함수로 거쳐 최종 예측값을 얻는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAwk4EDP66CH"
      },
      "source": [
        "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
        "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
        "output = Activation('sigmoid')(dot_product)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ziihyk0G68ZS"
      },
      "source": [
        "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0hcmP1q6-Nj"
      },
      "source": [
        "# 모델의 학습은 5에포크 수행\n",
        "for epoch in range(1, 6):\n",
        "    loss = 0\n",
        "    for _, elem in enumerate(skip_grams):\n",
        "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [first_elem, second_elem]\n",
        "        Y = labels\n",
        "        loss += model.train_on_batch(X,Y)  \n",
        "    print('Epoch :',epoch, 'Loss :',loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DX2C8zh6MYt"
      },
      "source": [
        "## 5. 결과 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzgDg0vn7LVg"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# 학습된 임베딩 벡터들을 vector.txt에 저장\n",
        "f = open('vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
        "vectors = model.get_weights()[0]\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjjFmWf_7Sjo"
      },
      "source": [
        "##### gensim의 .models.KeyedVectors.load_word2vec_format로 로드하면 쉽게 단어 간 유사도를 구할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdEH0q_Z7Yf_"
      },
      "source": [
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sej94pZq7fId"
      },
      "source": [
        "w2v.most_similar(positive=['soldiers'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEh_Mj_y7gS4"
      },
      "source": [
        "w2v.most_similar(positive=['doctor'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5lLt-lQ7iYV"
      },
      "source": [
        "w2v.most_similar(positive=['police'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2W1ChLg7kDX"
      },
      "source": [
        "w2v.most_similar(positive=['knife'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hGuIT8T7lll"
      },
      "source": [
        "w2v.most_similar(positive=['engine'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}